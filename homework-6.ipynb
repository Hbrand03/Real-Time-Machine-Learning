{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Problem 1","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport time\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ------------------------------\n# Vision Transformer Definition\n# ------------------------------\n\nclass ViT(nn.Module):\n    def __init__(self, img_size=32, patch_size=4, in_channels=3, num_classes=100,\n                 embed_dim=256, depth=6, num_heads=8, mlp_dim=512, dropout=0.1):\n        super(ViT, self).__init__()\n\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = (img_size // patch_size) ** 2\n        self.patch_dim = in_channels * patch_size * patch_size\n\n        self.patch_embed = nn.Linear(self.patch_dim, embed_dim)\n\n        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim))\n        self.dropout = nn.Dropout(dropout)\n\n        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads,\n                                                   dim_feedforward=mlp_dim, dropout=dropout, batch_first=True)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(embed_dim),\n            nn.Linear(embed_dim, num_classes)\n        )\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n\n        # Divide into patches\n        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n        x = x.contiguous().view(B, C, -1, self.patch_size, self.patch_size)\n        x = x.permute(0, 2, 1, 3, 4)  # B, num_patches, C, patch, patch\n        x = x.reshape(B, self.num_patches, -1)  # B, num_patches, patch_dim\n\n        # Patch embedding\n        x = self.patch_embed(x)  # (B, num_patches, embed_dim)\n\n        # Add class token\n        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, embed_dim)\n        x = torch.cat((cls_tokens, x), dim=1)  # (B, num_patches+1, embed_dim)\n\n        # Add positional encoding\n        x = x + self.pos_embed\n        x = self.dropout(x)\n\n        # Transformer\n        x = self.transformer(x)\n\n        # Classification using CLS token\n        cls_output = x[:, 0]\n        return self.mlp_head(cls_output)\n\n# ----------------------\n# CIFAR-100 DataLoader\n# ----------------------\n\ntransform_train = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32, padding=4),\n    transforms.ToTensor()\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor()\n])\n\ntrain_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\ntest_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n\n# ----------------------\n# Training Function\n# ----------------------\n\ndef train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs=20):\n    best_acc = 0.0\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss, running_corrects, total = 0.0, 0, 0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            _, preds = torch.max(outputs, 1)\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels)\n            total += labels.size(0)\n\n        train_loss = running_loss / total\n        train_acc = running_corrects.double() / total\n\n        model.eval()\n        val_corrects, val_total = 0, 0\n        with torch.no_grad():\n            for inputs, labels in test_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n                val_corrects += torch.sum(preds == labels)\n                val_total += labels.size(0)\n        val_acc = val_corrects.double() / val_total\n\n        scheduler.step()\n\n        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} Val Acc: {val_acc:.4f}\")\n        best_acc = max(best_acc, val_acc.item())\n\n    return best_acc\n\n# ----------------------\n# Model Init + Training\n# ----------------------\n\nmodel = ViT(img_size=32, patch_size=4, embed_dim=256, num_classes=100).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\nscheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n\nstart_time = time.time()\nbest_acc = train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs=20)\nend_time = time.time()\n\nprint(f\"\\nBest Validation Accuracy: {best_acc:.4f}\")\nprint(f\"Training Time: {(end_time - start_time)/60:.2f} minutes\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T03:35:06.828241Z","iopub.execute_input":"2025-04-19T03:35:06.828595Z","iopub.status.idle":"2025-04-19T03:43:57.292015Z","shell.execute_reply.started":"2025-04-19T03:35:06.828569Z","shell.execute_reply":"2025-04-19T03:43:57.291119Z"}},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nEpoch 1/20 - Train Loss: 4.3646 Acc: 0.0348 Val Acc: 0.0499\nEpoch 2/20 - Train Loss: 3.9552 Acc: 0.0845 Val Acc: 0.1067\nEpoch 3/20 - Train Loss: 3.7099 Acc: 0.1255 Val Acc: 0.1503\nEpoch 4/20 - Train Loss: 3.5415 Acc: 0.1526 Val Acc: 0.1815\nEpoch 5/20 - Train Loss: 3.3984 Acc: 0.1798 Val Acc: 0.1942\nEpoch 6/20 - Train Loss: 3.2914 Acc: 0.1986 Val Acc: 0.2144\nEpoch 7/20 - Train Loss: 3.2064 Acc: 0.2130 Val Acc: 0.2397\nEpoch 8/20 - Train Loss: 3.1180 Acc: 0.2305 Val Acc: 0.2411\nEpoch 9/20 - Train Loss: 3.0433 Acc: 0.2458 Val Acc: 0.2649\nEpoch 10/20 - Train Loss: 2.9754 Acc: 0.2580 Val Acc: 0.2653\nEpoch 11/20 - Train Loss: 2.8116 Acc: 0.2905 Val Acc: 0.3106\nEpoch 12/20 - Train Loss: 2.7568 Acc: 0.3002 Val Acc: 0.3132\nEpoch 13/20 - Train Loss: 2.7118 Acc: 0.3107 Val Acc: 0.3216\nEpoch 14/20 - Train Loss: 2.6653 Acc: 0.3192 Val Acc: 0.3216\nEpoch 15/20 - Train Loss: 2.6208 Acc: 0.3276 Val Acc: 0.3282\nEpoch 16/20 - Train Loss: 2.5814 Acc: 0.3332 Val Acc: 0.3427\nEpoch 17/20 - Train Loss: 2.5344 Acc: 0.3465 Val Acc: 0.3459\nEpoch 18/20 - Train Loss: 2.4952 Acc: 0.3533 Val Acc: 0.3488\nEpoch 19/20 - Train Loss: 2.4586 Acc: 0.3630 Val Acc: 0.3515\nEpoch 20/20 - Train Loss: 2.4235 Acc: 0.3705 Val Acc: 0.3563\n\nBest Validation Accuracy: 0.3563\nTraining Time: 8.81 minutes\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Problem 2","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom transformers import SwinForImageClassification, SwinConfig, AutoImageProcessor\nfrom transformers import TrainingArguments, Trainer\nimport time\n\n# Use GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define transforms for CIFAR-100 to match Swin input size (224x224)\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2761))\n])\n\n# Load CIFAR-100 datasets\ntrain_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\ntest_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n\n# DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n\n# Helper function to fine-tune model\ndef fine_tune_model(model_name):\n    model = SwinForImageClassification.from_pretrained(model_name, num_labels=100)\n    \n    # Freeze backbone\n    for param in model.swin.parameters():\n        param.requires_grad = False\n    \n    model = model.to(device)\n\n    # Training setup\n    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n    criterion = torch.nn.CrossEntropyLoss()\n\n    def train_epoch():\n        model.train()\n        total_loss = 0\n        correct = 0\n        total = 0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images).logits\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            preds = outputs.argmax(dim=1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n        return total_loss / len(train_loader), correct / total\n\n    def evaluate():\n        model.eval()\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for images, labels in test_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images).logits\n                preds = outputs.argmax(dim=1)\n                correct += (preds == labels).sum().item()\n                total += labels.size(0)\n        return correct / total\n\n    # Training loop\n    epoch_times = []\n    for epoch in range(2):  # Change to 5 for more epochs\n        start = time.time()\n        train_loss, train_acc = train_epoch()\n        end = time.time()\n        epoch_times.append(end - start)\n        print(f\"Epoch {epoch+1}, Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Time: {epoch_times[-1]:.2f}s\")\n\n    test_acc = evaluate()\n    return sum(epoch_times) / len(epoch_times), test_acc\n\n# Run for both Tiny and Small\ntiny_time, tiny_acc = fine_tune_model(\"microsoft/swin-tiny-patch4-window7-224\")\nsmall_time, small_acc = fine_tune_model(\"microsoft/swin-small-patch4-window7-224\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
